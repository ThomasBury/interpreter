{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6263b3f",
   "metadata": {},
   "source": [
    "# From log-units SHAP to natural units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_tweedie_deviance,\n",
    ")\n",
    "\n",
    "\n",
    "def load_mtpl2(n_samples=None):\n",
    "    \"\"\"Fetch the French Motor Third-Party Liability Claims dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples: int, default=None\n",
    "      number of samples to select (for faster run time). Full dataset has\n",
    "      678013 samples.\n",
    "    \"\"\"\n",
    "    # freMTPL2freq dataset from https://www.openml.org/d/41214\n",
    "    df_freq = fetch_openml(data_id=41214, as_frame=True).data\n",
    "    df_freq[\"IDpol\"] = df_freq[\"IDpol\"].astype(int)\n",
    "    df_freq.set_index(\"IDpol\", inplace=True)\n",
    "\n",
    "    # freMTPL2sev dataset from https://www.openml.org/d/41215\n",
    "    df_sev = fetch_openml(data_id=41215, as_frame=True).data\n",
    "\n",
    "    # sum ClaimAmount over identical IDs\n",
    "    df_sev = df_sev.groupby(\"IDpol\").sum()\n",
    "\n",
    "    df = df_freq.join(df_sev, how=\"left\")\n",
    "    df[\"ClaimAmount\"] = df[\"ClaimAmount\"].fillna(0)\n",
    "\n",
    "    # unquote string fields\n",
    "    for column_name in df.columns[[t is object for t in df.dtypes.values]]:\n",
    "        df[column_name] = df[column_name].str.strip(\"'\")\n",
    "    return df.iloc[:n_samples]\n",
    "\n",
    "\n",
    "def plot_obs_pred(\n",
    "    df,\n",
    "    feature,\n",
    "    weight,\n",
    "    observed,\n",
    "    predicted,\n",
    "    y_label=None,\n",
    "    title=None,\n",
    "    ax=None,\n",
    "    fill_legend=False,\n",
    "):\n",
    "    \"\"\"Plot observed and predicted - aggregated per feature level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        input data\n",
    "    feature: str\n",
    "        a column name of df for the feature to be plotted\n",
    "    weight : str\n",
    "        column name of df with the values of weights or exposure\n",
    "    observed : str\n",
    "        a column name of df with the observed target\n",
    "    predicted : DataFrame\n",
    "        a dataframe, with the same index as df, with the predicted target\n",
    "    fill_legend : bool, default=False\n",
    "        whether to show fill_between legend\n",
    "    \"\"\"\n",
    "    # aggregate observed and predicted variables by feature level\n",
    "    df_ = df.loc[:, [feature, weight]].copy()\n",
    "    df_[\"observed\"] = df[observed] * df[weight]\n",
    "    df_[\"predicted\"] = predicted * df[weight]\n",
    "    df_ = (\n",
    "        df_.groupby([feature])[[weight, \"observed\", \"predicted\"]]\n",
    "        .sum()\n",
    "        .assign(observed=lambda x: x[\"observed\"] / x[weight])\n",
    "        .assign(predicted=lambda x: x[\"predicted\"] / x[weight])\n",
    "    )\n",
    "\n",
    "    ax = df_.loc[:, [\"observed\", \"predicted\"]].plot(style=\".\", ax=ax)\n",
    "    y_max = df_.loc[:, [\"observed\", \"predicted\"]].values.max() * 0.8\n",
    "    p2 = ax.fill_between(\n",
    "        df_.index,\n",
    "        0,\n",
    "        y_max * df_[weight] / df_[weight].values.max(),\n",
    "        color=\"g\",\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    if fill_legend:\n",
    "        ax.legend([p2], [\"{} distribution\".format(feature)])\n",
    "    ax.set(\n",
    "        ylabel=y_label if y_label is not None else None,\n",
    "        title=title if title is not None else \"Train: Observed vs Predicted\",\n",
    "    )\n",
    "\n",
    "\n",
    "def score_estimator(\n",
    "    estimator,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    df_train,\n",
    "    df_test,\n",
    "    target,\n",
    "    weights,\n",
    "    tweedie_powers=None,\n",
    "):\n",
    "    \"\"\"Evaluate an estimator on train and test sets with different metrics\"\"\"\n",
    "\n",
    "    metrics = [\n",
    "        (\"DÂ² explained\", None),  # Use default scorer if it exists\n",
    "        (\"mean abs. error\", mean_absolute_error),\n",
    "        (\"mean squared error\", mean_squared_error),\n",
    "    ]\n",
    "    if tweedie_powers:\n",
    "        metrics += [\n",
    "            (\n",
    "                \"mean Tweedie dev p={:.4f}\".format(power),\n",
    "                partial(mean_tweedie_deviance, power=power),\n",
    "            )\n",
    "            for power in tweedie_powers\n",
    "        ]\n",
    "\n",
    "    res = []\n",
    "    for subset_label, X, df in [\n",
    "        (\"train\", X_train, df_train),\n",
    "        (\"test\", X_test, df_test),\n",
    "    ]:\n",
    "        y, _weights = df[target], df[weights]\n",
    "        for score_label, metric in metrics:\n",
    "            if isinstance(estimator, tuple) and len(estimator) == 2:\n",
    "                # Score the model consisting of the product of frequency and\n",
    "                # severity models.\n",
    "                est_freq, est_sev = estimator\n",
    "                y_pred = est_freq.predict(X) * est_sev.predict(X)\n",
    "            else:\n",
    "                y_pred = estimator.predict(X)\n",
    "\n",
    "            if metric is None:\n",
    "                if not hasattr(estimator, \"score\"):\n",
    "                    continue\n",
    "                score = estimator.score(X, y, sample_weight=_weights)\n",
    "            else:\n",
    "                score = metric(y, y_pred, sample_weight=_weights)\n",
    "\n",
    "            res.append({\"subset\": subset_label, \"metric\": score_label, \"score\": score})\n",
    "\n",
    "    res = (\n",
    "        pd.DataFrame(res)\n",
    "        .set_index([\"metric\", \"subset\"])\n",
    "        .score.unstack(-1)\n",
    "        .round(4)\n",
    "        .loc[:, [\"train\", \"test\"]]\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb525275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ClaimNb  Exposure Area  VehPower  VehAge  DrivAge  BonusMalus VehBrand  \\\n",
      "IDpol                                                                           \n",
      "139          1      0.75    F         7       1       61          50      B12   \n",
      "190          1      0.14    B        12       5       50          60      B12   \n",
      "414          1      0.14    E         4       0       36          85      B12   \n",
      "424          2      0.62    F        10       0       51         100      B12   \n",
      "463          1      0.31    A         5       0       45          50      B12   \n",
      "\n",
      "          VehGas  Density Region  ClaimAmount   PurePremium  Frequency  \\\n",
      "IDpol                                                                    \n",
      "139    'Regular'    27000    R11       303.00    404.000000   1.333333   \n",
      "190     'Diesel'       56    R25      1981.84  14156.000000   7.142857   \n",
      "414    'Regular'     4792    R11      1456.55  10403.928571   7.142857   \n",
      "424    'Regular'    27000    R11     10834.00  17474.193548   3.225806   \n",
      "463    'Regular'       12    R73      3986.67  12860.225806   3.225806   \n",
      "\n",
      "       AvgClaimAmount  \n",
      "IDpol                  \n",
      "139            303.00  \n",
      "190           1981.84  \n",
      "414           1456.55  \n",
      "424           5417.00  \n",
      "463           3986.67  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    KBinsDiscretizer,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "\n",
    "df = load_mtpl2()\n",
    "\n",
    "\n",
    "# Correct for unreasonable observations (that might be data error)\n",
    "# and a few exceptionally large claim amounts\n",
    "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
    "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
    "df[\"ClaimAmount\"] = df[\"ClaimAmount\"].clip(upper=200000)\n",
    "# If the claim amount is 0, then we do not count it as a claim. The loss function\n",
    "# used by the severity model needs strictly positive claim amounts. This way\n",
    "# frequency and severity are more consistent with each other.\n",
    "df.loc[(df[\"ClaimAmount\"] == 0) & (df[\"ClaimNb\"] >= 1), \"ClaimNb\"] = 0\n",
    "\n",
    "log_scale_transformer = make_pipeline(\n",
    "    FunctionTransformer(func=np.log), StandardScaler()\n",
    ")\n",
    "\n",
    "column_trans = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"binned_numeric\",\n",
    "            KBinsDiscretizer(\n",
    "                n_bins=10, quantile_method=\"averaged_inverted_cdf\", random_state=0\n",
    "            ),\n",
    "            [\"VehAge\", \"DrivAge\"],\n",
    "        ),\n",
    "        (\n",
    "            \"onehot_categorical\",\n",
    "            OneHotEncoder(),\n",
    "            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"],\n",
    "        ),\n",
    "        (\"passthrough_numeric\", \"passthrough\", [\"BonusMalus\"]),\n",
    "        (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"]),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "X = column_trans.fit_transform(df)\n",
    "\n",
    "# Insurances companies are interested in modeling the Pure Premium, that is\n",
    "# the expected total claim amount per unit of exposure for each policyholder\n",
    "# in their portfolio:\n",
    "df[\"PurePremium\"] = df[\"ClaimAmount\"] / df[\"Exposure\"]\n",
    "\n",
    "# This can be indirectly approximated by a 2-step modeling: the product of the\n",
    "# Frequency times the average claim amount per claim:\n",
    "df[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\n",
    "df[\"AvgClaimAmount\"] = df[\"ClaimAmount\"] / np.fmax(df[\"ClaimNb\"], 1)\n",
    "\n",
    "with pd.option_context(\"display.max_columns\", 15):\n",
    "    print(df[df.ClaimAmount > 0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91edc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, X_train, X_test = train_test_split(df, X, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea3bc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the Product Model and the Tweedie Regressor on target PurePremium\n",
      "                          Product Model              \n",
      "subset                            train          test\n",
      "metric                                               \n",
      "DÂ² explained               1.640000e-02  1.370000e-02\n",
      "mean Tweedie dev p=1.1000  6.590273e+02  6.496857e+02\n",
      "mean Tweedie dev p=1.2500  2.683968e+02  2.664248e+02\n",
      "mean Tweedie dev p=1.5000  7.640460e+01  7.641980e+01\n",
      "mean Tweedie dev p=1.7000  3.682720e+01  3.692730e+01\n",
      "mean abs. error            2.740176e+02  2.731633e+02\n",
      "mean squared error         3.295518e+07  3.213087e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99860/2754076991.py:21: FutureWarning: The behavior of pd.concat with len(keys) != len(objs) is deprecated. In a future version this will raise instead of truncating to the smaller of the two sequences\n",
      "  scores = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "glm_pure_premium = TweedieRegressor(power=1.9, alpha=0.1, solver=\"newton-cholesky\")\n",
    "glm_pure_premium.fit(\n",
    "    X_train, df_train[\"PurePremium\"], sample_weight=df_train[\"Exposure\"]\n",
    ")\n",
    "\n",
    "tweedie_powers = [1.1, 1.25, 1.5, 1.7]\n",
    "\n",
    "scores_glm_pure_premium = score_estimator(\n",
    "    glm_pure_premium,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    df_train,\n",
    "    df_test,\n",
    "    target=\"PurePremium\",\n",
    "    weights=\"Exposure\",\n",
    "    tweedie_powers=tweedie_powers,\n",
    ")\n",
    "\n",
    "scores = pd.concat(\n",
    "    [scores_glm_pure_premium],\n",
    "    axis=1,\n",
    "    sort=True,\n",
    "    keys=(\"Product Model\", \"TweedieRegressor\"),\n",
    ")\n",
    "print(\"Evaluation of the Product Model and the Tweedie Regressor on target PurePremium\")\n",
    "with pd.option_context(\"display.expand_frame_repr\", False):\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "532abe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset                                 train          test\n",
      "observed                        3.917618e+07  1.299546e+07\n",
      "predicted, tweedie, power=1.90  3.952914e+07  1.325666e+07\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for subset_label, X, df in [\n",
    "    (\"train\", X_train, df_train),\n",
    "    (\"test\", X_test, df_test),\n",
    "]:\n",
    "    exposure = df[\"Exposure\"].values\n",
    "    res.append(\n",
    "        {\n",
    "            \"subset\": subset_label,\n",
    "            \"observed\": df[\"ClaimAmount\"].values.sum(),\n",
    "            \"predicted, tweedie, power=%.2f\" % glm_pure_premium.power: np.sum(\n",
    "                exposure * glm_pure_premium.predict(X)\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(pd.DataFrame(res).set_index(\"subset\").T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269deaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Î¼_i: [ 88.31531587  76.75336521  87.04875287 114.22201482 116.51736045]\n",
      "Directly predicted Î¼_i: [ 88.31531587  76.75336521  87.04875287 114.22201482 116.51736045]\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Fit the model\n",
    "glm_pure_premium = TweedieRegressor(power=1.5, alpha=0.1, solver=\"newton-cholesky\")\n",
    "glm_pure_premium.fit(\n",
    "    X_train,\n",
    "    df_train[\"PurePremium\"],\n",
    "    sample_weight=df_train[\"Exposure\"],\n",
    ")\n",
    "\n",
    "# Build a LinearExplainer for the logâlink Î· = XÎ²\n",
    "explainer = shap.LinearExplainer(\n",
    "    glm_pure_premium,\n",
    "    X_train,\n",
    ")\n",
    "\n",
    "# Get raw SHAP on Î· for X_test\n",
    "shap_values_loglink = explainer.shap_values(X_test)\n",
    "shap_multipliers = np.exp(shap_values_loglink)\n",
    "base_value_log = explainer.expected_value      # E[Î·], scalar\n",
    "mu_baseline = np.exp(base_value_log)         # E[Î¼], the baseline (EUR)\n",
    "\n",
    "# For each test sample i (scikit-learn already applies the link function internally),\n",
    "mu_test = glm_pure_premium.predict(X_test) \n",
    "\n",
    "# Reconstruct Î¼_i from baseline Ã â_j multipliers\n",
    "mu_reconstructed = mu_baseline * np.prod(shap_multipliers, axis=1)\n",
    "\n",
    "print(f\"Reconstructed Î¼_i: {mu_reconstructed[:5]}\")\n",
    "print(f\"Directly predicted Î¼_i: {mu_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f1f02",
   "metadata": {},
   "source": [
    "## From multiplicative to additive attribution\n",
    "\n",
    "Exact additive attribution but path dependent, depends on the predictors order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd2a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max discrepancy (EUR): 1.1641532182693481e-10\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# copy the SHAP object (so you donât overwrite the original)\n",
    "shap_exp_centered = copy.deepcopy(shap_values_loglink)\n",
    "\n",
    "# convert the baseline from log-scale to EUR:\n",
    "base_value = np.exp(explainer.expected_value)\n",
    "# Now base_values[i] = exp( base_eta_i ).  Usually base_values is a length-n array,\n",
    "# but often all elements are identical = explainer.expected_value.  \n",
    "\n",
    "# turn each log-SHAP Ï_{ij}^{(log)} into a multiplier exp(Ï_{ij}^{(log)}).  \n",
    "# Then do cumulative product across features (axis=1), scaling by the baseline:\n",
    "shap_exp_centered = np.cumprod(\n",
    "    np.exp(shap_exp_centered), axis=1\n",
    ") * base_value\n",
    "#    shap_exp_centered.values[i, k] now = Î¼â Ã â_{j=1..k} exp(Ï_{ij}^{(log)})\n",
    "#    which is exactly âmodelâs prediction after including features 1..k.â\n",
    "\n",
    "# Convert that into per-feature âadditive â¬ differencesâ:\n",
    "target_diff = np.diff(shap_exp_centered, axis=1)\n",
    "# target_diff[i, k-1] =  Î¼âÃâ_{j=1..k}exp(Ï_{ij})  â Î¼âÃâ_{j=1..k-1}exp(Ï_{ij})\n",
    "\n",
    "# For the very first feature, you need Î of (step1 â baseline):\n",
    "# zero_value = np.zeros((len(base_value), 1))\n",
    "first_contrib_minus_base = (\n",
    "    shap_exp_centered[:, [0]]  # âÎ¼âÃexp(Ï_{i,1})â\n",
    "    - base_value  # âÎ¼ââ\n",
    ")\n",
    "\n",
    "# Rebuild shap_exp_centered.values as \n",
    "# [Î_feature1, Î_feature2, â¦, Î_feature_p] for each row i:\n",
    "shap_exp_centered = np.concatenate(\n",
    "    (first_contrib_minus_base, target_diff), axis=1\n",
    ")\n",
    "\n",
    "# Sum up the perâfeature ââ¬ contributionsâ for each sample:\n",
    "sum_contribs = np.sum(shap_exp_centered, axis=1)  # shape = (n_test,)\n",
    "\n",
    "# Add back the baseline (in EUR) to reconstruct the predicted pureâpremium:\n",
    "reconstructed_mu = base_value + sum_contribs\n",
    "# shap_exp_centered.base_values[i] should already be exp(original_log_base).\n",
    "\n",
    "# Get the modelâs predicted Î¼ in EUR directly:\n",
    "predicted_mu = glm_pure_premium.predict(X_test)  # this is exp(Î·_i) = Î¼_i\n",
    "\n",
    "# Compare themâideally they match up to machineâprecision:\n",
    "diff = np.abs(reconstructed_mu - predicted_mu)\n",
    "\n",
    "print(\"Max discrepancy (EUR):\", diff.max())\n",
    "if not np.allclose(reconstructed_mu, predicted_mu, rtol=1e-6, atol=1e-8):\n",
    "    raise ValueError(\n",
    "        \"Sum of SHAPâbased contributions + baseline \"\n",
    "        \"does not match model prediction (EUR)!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additivity check passed. Max discrepancy: 70546.10494107334\n"
     ]
    }
   ],
   "source": [
    "# Firstâorder (Taylor) approximation: convert each logâSHAP Ï_{i,j} into an additive â¬ amount\n",
    "\n",
    "# Convert the baseline from logâscale to EUR:\n",
    "base_value_log = explainer.expected_value       \n",
    "base_value_EUR = np.exp(base_value_log)        \n",
    "shap_values_EUR = shap_values_loglink * base_value_EUR\n",
    "\n",
    "# Additivity is usually not met\n",
    "# Î¼_i â base_value_EUR + sum_j ÎÎ¼_{i,j}\n",
    "reconstructed_mu = base_value_EUR + np.sum(shap_values_EUR, axis=1)\n",
    "print(\"Additivity check passed. Max discrepancy:\", np.max(np.abs(mu_test - reconstructed_mu)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
